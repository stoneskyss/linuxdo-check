# -*- coding: utf-8 -*-
"""
cron: 0 */6 * * *
new Env("Linux.Do ç­¾åˆ°")
"""

import os
import random
import time
import functools
import re
import tempfile
from loguru import logger
from DrissionPage import ChromiumOptions, Chromium
from tabulate import tabulate
from curl_cffi import requests
from bs4 import BeautifulSoup


# ----------------------------
# Retry Decorator
# ----------------------------
def retry_decorator(retries=3, min_delay=5, max_delay=10):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_err = None
            for attempt in range(retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_err = e
                    if attempt == retries - 1:
                        logger.error(f"å‡½æ•° {func.__name__} æœ€ç»ˆæ‰§è¡Œå¤±è´¥: {str(e)}")
                    else:
                        logger.warning(
                            f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1}/{retries} æ¬¡å°è¯•å¤±è´¥: {str(e)}"
                        )
                        sleep_s = random.uniform(min_delay, max_delay)
                        logger.info(
                            f"å°†åœ¨ {sleep_s:.2f}s åé‡è¯• ({min_delay}-{max_delay}s éšæœºå»¶è¿Ÿ)"
                        )
                        time.sleep(sleep_s)
            raise last_err

        return wrapper

    return decorator


# ----------------------------
# Env & Config
# ----------------------------
# æ³¨æ„ï¼šä¸è¦å¼ºè¡Œ pop DISPLAYï¼›ä½ åœ¨ Actions + Xvfb æ—¶éœ€è¦å®ƒ
os.environ.pop("DYLD_LIBRARY_PATH", None)

USERNAME = os.environ.get("LINUXDO_USERNAME") or os.environ.get("USERNAME")
PASSWORD = os.environ.get("LINUXDO_PASSWORD") or os.environ.get("PASSWORD")

BROWSE_ENABLED = os.environ.get("BROWSE_ENABLED", "true").strip().lower() not in [
    "false",
    "0",
    "off",
]

# âœ… Actions + Xvfb æ—¶å¯è®¾ HEADLESS=false
HEADLESS = os.environ.get("HEADLESS", "true").strip().lower() not in ["false", "0", "off"]

# æ¯æ¬¡è¿è¡Œæœ€å¤šè¿›å…¥å¤šå°‘ä¸ªè¯é¢˜å¸–
MAX_TOPICS = int(os.environ.get("MAX_TOPICS", "50"))

# æ¯ä¸ªè¯é¢˜è‡³å°‘/æœ€å¤šæµè§ˆå¤šå°‘â€œé¡µ/æ‰¹æ¬¡â€è¯„è®º
MIN_COMMENT_PAGES = int(os.environ.get("MIN_COMMENT_PAGES", "5"))
MAX_COMMENT_PAGES = int(os.environ.get("MAX_COMMENT_PAGES", "10"))

# â€œç¿»ä¸€é¡µè¯„è®ºâ€çš„åˆ¤å®šï¼šæœ€å¤§æ¥¼å±‚å·å¢é•¿å¤šå°‘ç®— 1 é¡µ
PAGE_GROW = int(os.environ.get("PAGE_GROW", "10"))

# ç‚¹èµæ¦‚ç‡ï¼ˆ0~1ï¼‰
LIKE_PROB = float(os.environ.get("LIKE_PROB", "0.3"))

# å¤§æ­¥æ»šåŠ¨è·ç¦»ï¼ˆæ¨è¿›æ¥¼å±‚å¢é•¿ï¼‰
SCROLL_MIN = int(os.environ.get("SCROLL_MIN", "1000"))
SCROLL_MAX = int(os.environ.get("SCROLL_MAX", "1600"))

# âœ… å€Ÿé‰´ä½ â€œå¯ç”¨è„šæœ¬â€çš„æ»šåŠ¨èŠ‚å¥ï¼ˆæ›´åƒçœŸäººï¼‰
READ_SCROLL_MIN = int(os.environ.get("READ_SCROLL_MIN", "200"))
READ_SCROLL_MAX = int(os.environ.get("READ_SCROLL_MAX", "500"))
READ_INTERVAL_MIN = float(os.environ.get("READ_INTERVAL_MIN", "1"))
READ_INTERVAL_MAX = float(os.environ.get("READ_INTERVAL_MAX", "3"))

# âœ… é˜…è¯»åœç•™ï¼ˆé»˜è®¤å†™æ­» 5 / 20ï¼Œä½ ä¹Ÿå¯ env è¦†ç›–ï¼‰
MIN_READ_STAY = float(os.environ.get("MIN_READ_STAY", "5"))
READ_STATE_TIMEOUT = float(os.environ.get("READ_STATE_TIMEOUT", "20"))

# æ¥è¿‘åº•éƒ¨åˆ¤å®š
NEAR_BOTTOM_GAP = int(os.environ.get("NEAR_BOTTOM_GAP", "140"))
BOTTOM_EXTRA_STAY_MIN = float(os.environ.get("BOTTOM_EXTRA_STAY_MIN", "6"))
BOTTOM_EXTRA_STAY_MAX = float(os.environ.get("BOTTOM_EXTRA_STAY_MAX", "12"))

# âœ… timingsï¼ˆæŒ‰ä½ æ‰©å±• content.js çš„é»˜è®¤å€¼ï¼‰
TIMINGS_MIN_REQ = int(os.environ.get("TIMINGS_MIN_REQ", "8"))
TIMINGS_MAX_REQ = int(os.environ.get("TIMINGS_MAX_REQ", "20"))
TIMINGS_MIN_MS = int(os.environ.get("TIMINGS_MIN_MS", "800"))
TIMINGS_MAX_MS = int(os.environ.get("TIMINGS_MAX_MS", "3000"))
TIMINGS_BASE_DELAY_MS = int(os.environ.get("TIMINGS_BASE_DELAY_MS", "2500"))
TIMINGS_RANDOM_DELAY_MS = int(os.environ.get("TIMINGS_RANDOM_DELAY_MS", "800"))

# âœ… åªå¯¹â€œä»æœ‰è“ç‚¹â€çš„æ¥¼å±‚åš timingsï¼ˆæ›´ç¨³ï¼‰
ONLY_TIMINGS_UNREAD = os.environ.get("ONLY_TIMINGS_UNREAD", "true").strip().lower() not in [
    "false",
    "0",
    "off",
]

# DrissionPage è¿œç¨‹è°ƒè¯•ç«¯å£ï¼šé¿å… Actions é‡Œ 9222 å†²çª
DP_PORT = int(os.environ.get("DP_PORT", str(random.randint(20000, 40000))))

GOTIFY_URL = os.environ.get("GOTIFY_URL")
GOTIFY_TOKEN = os.environ.get("GOTIFY_TOKEN")
SC3_PUSH_KEY = os.environ.get("SC3_PUSH_KEY")
WXPUSH_URL = os.environ.get("WXPUSH_URL")
WXPUSH_TOKEN = os.environ.get("WXPUSH_TOKEN")

# è®¿é—®å…¥å£
LIST_URL = "https://linux.do/latest"
HOME_FOR_COOKIE = "https://linux.do/"
LOGIN_URL = "https://linux.do/login"
SESSION_URL = "https://linux.do/session"
CSRF_URL = "https://linux.do/session/csrf"
TIMINGS_URL = "https://linux.do/topics/timings"

# å¸–å­æ­£æ–‡é€‰æ‹©å™¨ï¼ˆç”¨äºç¡®è®¤å·²æ¸²æŸ“ï¼‰
POST_CONTENT_CSS = "div.post__regular.regular.post__contents.contents"


class LinuxDoBrowser:
    def __init__(self) -> None:
        from sys import platform

        if platform.startswith("linux"):
            platformIdentifier = "X11; Linux x86_64"
        elif platform == "darwin":
            platformIdentifier = "Macintosh; Intel Mac OS X 10_15_7"
        elif platform == "win32":
            platformIdentifier = "Windows NT 10.0; Win64; x64"
        else:
            platformIdentifier = "X11; Linux x86_64"

        co = ChromiumOptions().incognito(True)

        # âœ… é¿å… 9222 å†²çª / user-data å†²çª
        user_dir = tempfile.mkdtemp(prefix="dp_ud_")
        co.set_user_data_path(user_dir)
        co.set_local_port(DP_PORT)

        # âœ… å¸¸è§ Actions/å®¹å™¨å‚æ•°
        co.set_argument("--no-sandbox")
        co.set_argument("--disable-dev-shm-usage")
        co.set_argument("--disable-gpu")
        co.set_argument("--disable-blink-features=AutomationControlled")
        co.set_argument("--disable-background-timer-throttling")
        co.set_argument("--disable-backgrounding-occluded-windows")
        co.set_argument("--disable-renderer-backgrounding")

        # æ— å¤´æ§åˆ¶
        co.headless(HEADLESS)
        if not HEADLESS:
            co.set_argument("--start-maximized")

        co.set_user_agent(
            f"Mozilla/5.0 ({platformIdentifier}) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36"
        )

        self.browser = Chromium(co)
        self.page = self.browser.new_tab()

        # requests session ç”¨äºç™»å½•/é€šçŸ¥ï¼Œä¸ç”¨äº timingsï¼ˆtimings å¿…é¡»èµ°æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼‰
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
                "Accept": "application/json, text/javascript, */*; q=0.01",
                "Accept-Language": "zh-CN,zh;q=0.9",
            }
        )

        # timings ç»Ÿè®¡
        self.timings_sent = 0
        self.timings_ok = 0
        self.timings_fail = 0

    # ----------------------------
    # Headers
    # ----------------------------
    def _api_headers(self):
        return {
            "User-Agent": self.session.headers.get("User-Agent"),
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "Accept-Language": "zh-CN,zh;q=0.9",
            "X-Requested-With": "XMLHttpRequest",
            "Referer": LOGIN_URL,
            "Origin": "https://linux.do",
        }

    def _html_headers(self):
        return {
            "User-Agent": self.session.headers.get("User-Agent"),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9",
            "Referer": HOME_FOR_COOKIE,
        }

    # ----------------------------
    # CSRF + Login (requests)
    # ----------------------------
    def _get_csrf_token(self) -> str:
        self.session.get(
            HOME_FOR_COOKIE,
            headers=self._html_headers(),
            impersonate="chrome136",
            allow_redirects=True,
            timeout=30,
        )

        resp_csrf = self.session.get(
            CSRF_URL,
            headers=self._api_headers(),
            impersonate="chrome136",
            allow_redirects=True,
            timeout=30,
        )
        ct = (resp_csrf.headers.get("content-type") or "").lower()
        if resp_csrf.status_code != 200 or "application/json" not in ct:
            head = (resp_csrf.text or "")[:200]
            raise RuntimeError(
                f"CSRF not JSON. status={resp_csrf.status_code}, ct={ct}, head={head}"
            )
        data = resp_csrf.json()
        csrf = data.get("csrf")
        if not csrf:
            raise RuntimeError(f"CSRF JSON missing token keys: {list(data.keys())}")
        return csrf

    def login(self):
        logger.info("å¼€å§‹ç™»å½•")
        logger.info("è·å– CSRF token...")

        try:
            csrf_token = self._get_csrf_token()
        except Exception as e:
            logger.error(f"è·å– CSRF å¤±è´¥ï¼š{e}")
            return False

        logger.info("æ­£åœ¨ç™»å½•...")

        headers = self._api_headers()
        headers.update(
            {
                "X-CSRF-Token": csrf_token,
                "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            }
        )

        data = {"login": USERNAME, "password": PASSWORD, "timezone": "Asia/Shanghai"}

        try:
            resp_login = self.session.post(
                SESSION_URL,
                data=data,
                impersonate="chrome136",
                headers=headers,
                allow_redirects=True,
                timeout=30,
            )
            ct = (resp_login.headers.get("content-type") or "").lower()
            if "application/json" not in ct:
                logger.error(f"ç™»å½•è¿”å›ä¸æ˜¯ JSONï¼Œhead={resp_login.text[:200]}")
                return False

            response_json = resp_login.json()
            if response_json.get("error"):
                logger.error(f"ç™»å½•å¤±è´¥: {response_json.get('error')}")
                return False

            logger.info("ç™»å½•æˆåŠŸ!")
        except Exception as e:
            logger.error(f"ç™»å½•è¯·æ±‚å¼‚å¸¸: {e}")
            return False

        self.print_connect_info()

        # åŒæ­¥ Cookie åˆ° DrissionPage
        logger.info("åŒæ­¥ Cookie åˆ° DrissionPage...")
        cookies_dict = self.session.cookies.get_dict()
        dp_cookies = [
            {"name": name, "value": value, "domain": ".linux.do", "path": "/"}
            for name, value in cookies_dict.items()
        ]
        self.page.set.cookies(dp_cookies)

        logger.info("Cookie è®¾ç½®å®Œæˆï¼Œå¯¼èˆªè‡³ä¸»é¢˜åˆ—è¡¨é¡µ /latest ...")
        self.page.get(LIST_URL)

        try:
            self.page.wait.ele("@id=main-outlet", timeout=25)
        except Exception:
            logger.warning("æœªç­‰åˆ° main-outletï¼Œä½†ç»§ç»­å°è¯•æŸ¥æ‰¾ topic link")

        ok = self._wait_any_topic_link(timeout=35)
        if not ok:
            logger.warning("æœªç­‰åˆ°ä¸»é¢˜é“¾æ¥ a.raw-topic-link")
            logger.warning(f"url={self.page.url}")
            logger.warning((self.page.html or "")[:500])
            return True

        logger.info("ä¸»é¢˜åˆ—è¡¨å·²æ¸²æŸ“ï¼Œç™»å½•&é¡µé¢åŠ è½½å®Œæˆ")
        return True

    def _wait_any_topic_link(self, timeout=30) -> bool:
        end = time.time() + timeout
        while time.time() < end:
            try:
                links = self.page.eles("css:a.raw-topic-link")
                if links and len(links) > 0:
                    return True
            except Exception:
                pass
            time.sleep(0.8)
        return False

    # ----------------------------
    # Topic/Posts helpers
    # ----------------------------
    def _post_count_in_dom(self, page) -> int:
        try:
            return int(page.run_js("""return document.querySelectorAll('[id^="post_"]').length;""") or 0)
        except Exception:
            return 0

    def _max_post_number_in_dom(self, page) -> int:
        try:
            return int(
                page.run_js(
                    """
                    let maxN = 0;
                    document.querySelectorAll('[id^="post_"]').forEach(el => {
                      const m = el.id.match(/^post_(\\d+)$/);
                      if (m) maxN = Math.max(maxN, parseInt(m[1], 10));
                    });
                    return maxN;
                    """
                )
                or 0
            )
        except Exception:
            return 0

    def wait_topic_posts_ready(self, page, timeout=60) -> bool:
        """
        âœ… ä¸ä¾èµ– #post_1ï¼šåªè¦ä»»æ„ post æœ‰æ­£æ–‡æ–‡æœ¬å³è§†ä¸º ready
        """
        end = time.time() + timeout
        while time.time() < end:
            try:
                ok = page.run_js(
                    f"""
                    const posts = Array.from(document.querySelectorAll('[id^="post_"]'));
                    if (!posts.length) return false;
                    for (const p of posts) {{
                      const c = p.querySelector('{POST_CONTENT_CSS}');
                      if (!c) continue;
                      const t = (c.innerText || c.textContent || '').trim();
                      if (t.length > 0) return true;
                    }}
                    return false;
                    """
                )
                if ok:
                    cnt = self._post_count_in_dom(page)
                    mx = self._max_post_number_in_dom(page)
                    mn = int(
                        page.run_js(
                            """
                            let minN = 999999;
                            document.querySelectorAll('[id^="post_"]').forEach(el=>{
                              const m = el.id.match(/^post_(\\d+)$/);
                              if (m) minN = Math.min(minN, parseInt(m[1],10));
                            });
                            return (minN===999999)?0:minN;
                            """
                        )
                        or 0
                    )
                    logger.info(f"å¸–å­æµå·²æ¸²æŸ“ï¼šdom_posts={cnt} range=post_{mn}..post_{mx}")
                    time.sleep(random.uniform(0.8, 1.6))
                    return True
            except Exception:
                pass
            time.sleep(0.6)

        logger.warning("æœªç­‰åˆ°å¸–å­æµæ¸²æŸ“å®Œæˆï¼ˆå¯èƒ½ç»“æ„å˜åŒ–/åŠ è½½æ…¢/è¢«æ‹¦æˆªï¼‰")
        return False

    # ----------------------------
    # Blue-dot / read-state helpers
    # ----------------------------
    def _post_has_blue_dot(self, page, post_id: int) -> bool:
        """
        è“ç‚¹åˆ¤æ–­ï¼šå­˜åœ¨ .read-state ä¸”ä¸åŒ…å« class 'read' => æœªè¯»
        """
        try:
            return bool(
                page.run_js(
                    """
                    const pid = arguments[0];
                    const root = document.querySelector(`#post_${pid}`);
                    if (!root) return false;
                    const rs = root.querySelector('.topic-meta-data .read-state');
                    if (!rs) return false;
                    return !rs.classList.contains('read');
                    """,
                    post_id,
                )
            )
        except Exception:
            return False

    def _list_visible_posts_in_viewport(self, page):
        try:
            ids = page.run_js(
                """
                const els = Array.from(document.querySelectorAll('[id^="post_"]'));
                const v = [];
                for (const el of els) {
                  const r = el.getBoundingClientRect();
                  if (r.bottom < 0 || r.top > window.innerHeight) continue;
                  const m = el.id.match(/^post_(\\d+)$/);
                  if (m) v.push(parseInt(m[1], 10));
                }
                return v;
                """
            )
            if not ids:
                return []
            return [int(x) for x in ids]
        except Exception:
            return []

    # ----------------------------
    # timings (æŒ‰æ‰©å±• content.js é€»è¾‘ï¼šåœ¨æµè§ˆå™¨ä¸Šä¸‹æ–‡ fetch + headers + form)
    # ----------------------------
    def _get_topic_id_and_csrf(self, page):
        """
        è¿”å› (topic_id:int|None, csrf:str|None)
        topic_idï¼šæŒ‰ä½ æ‰©å±•é‡Œçš„æ–¹å¼ä» pathname split å–ç¬¬ 4 æ®µ
        """
        try:
            data = page.run_js(
                """
                const csrfEl = document.querySelector('meta[name="csrf-token"]');
                const csrf = csrfEl ? csrfEl.getAttribute('content') : null;
                const parts = window.location.pathname.split('/').filter(Boolean);
                // å¸¸è§ï¼š/t/topic/1564445/29 => parts = ['t','topic','1564445','29']
                const tid = (parts.length >= 3) ? parseInt(parts[2], 10) : null;
                return {topic_id: tid, csrf: csrf, url: location.href};
                """
            )
            if not data:
                return None, None, None
            return data.get("topic_id"), data.get("csrf"), data.get("url")
        except Exception:
            return None, None, None

    def _post_timings_via_page_fetch(self, page, post_ids):
        """
        âœ… æ¨¡ä»¿æ‰©å±•ï¼šPOST /topics/timings
        - headers: accept */*, x-csrf-token, discourse-present/background/logged-in, x-requested-with, x-silence-logger
        - body: timings[pid]=éšæœºæ¯«ç§’ + topic_time + topic_id
        - referrer: å½“å‰ topic é¡µé¢ URLï¼ˆæ›´åƒçœŸå®ï¼‰
        """
        post_ids = [int(x) for x in post_ids if isinstance(x, (int, str)) and str(x).isdigit()]
        post_ids = sorted(set(post_ids))
        if not post_ids:
            return None

        topic_id, csrf, ref_url = self._get_topic_id_and_csrf(page)
        if not topic_id or not csrf or not ref_url:
            logger.warning("timings(fetch): æ— æ³•è·å– topic_id/csrf/ref_urlï¼Œè·³è¿‡")
            return None

        # ç”Ÿæˆ timings å€¼ï¼ˆå€Ÿé‰´æ‰©å±•ï¼šæ¯æ¥¼éšæœº min~maxï¼‰
        timings_map = {pid: random.randint(TIMINGS_MIN_MS, TIMINGS_MAX_MS) for pid in post_ids}
        topic_time = sum(timings_map.values())

        # ç»„è£… bodyï¼ˆæŒ‰ formï¼‰
        body_pairs = []
        for pid, ms in timings_map.items():
            body_pairs.append(f"timings[{pid}]={ms}")
        body_pairs.append(f"topic_time={topic_time}")
        body_pairs.append(f"topic_id={topic_id}")
        body = "&".join(body_pairs)

        # è®©æ¯æ¬¡è¯·æ±‚å¤§å°æ›´åƒæ‰©å±•ï¼šå¦‚æœä¼ å…¥ post_ids è¿‡å°‘ï¼Œå¯â€œå¡«å……â€åˆ° 1~Nï¼ˆä½†åªåœ¨ä½ å…è®¸æ—¶ï¼‰
        # è¿™é‡Œé»˜è®¤ä¸å¡«å……ï¼Œåªæäº¤ä½ è¿™æ¬¡é˜…è¯»æ¶‰åŠçš„æ¥¼å±‚

        js = """
        return (async () => {
          try {
            const url = arguments[0];
            const body = arguments[1];
            const csrf = arguments[2];
            const ref = arguments[3];

            const resp = await fetch(url, {
              method: "POST",
              mode: "cors",
              credentials: "include",
              referrer: ref,
              headers: {
                "accept": "*/*",
                "content-type": "application/x-www-form-urlencoded; charset=UTF-8",
                "discourse-background": "true",
                "discourse-logged-in": "true",
                "discourse-present": "true",
                "x-csrf-token": csrf,
                "x-requested-with": "XMLHttpRequest",
                "x-silence-logger": "true"
              },
              body
            });

            let head = "";
            try { head = (await resp.text()).slice(0, 160); } catch(e) {}
            return {ok: resp.ok, status: resp.status, head: head};
          } catch (e) {
            return {ok: false, status: -1, head: String(e).slice(0,160)};
          }
        })();
        """

        result = None
        try:
            result = page.run_js(js, TIMINGS_URL, body, csrf, ref_url)
        except Exception as e:
            result = {"ok": False, "status": -2, "head": str(e)[:160]}

        self.timings_sent += 1
        ok = bool(result and result.get("ok"))
        status = result.get("status") if result else None
        head = (result.get("head") if result else "") or ""
        if ok:
            self.timings_ok += 1
        else:
            self.timings_fail += 1

        logger.info(
            f"timings(fetch): status={status} ok={1 if ok else 0} "
            f"topic_id={topic_id} posts={post_ids} topic_time={topic_time} body={body}"
        )
        if not ok and head:
            logger.info(f"timings(fetch): head={head}")

        # æ¨¡ä»¿æ‰©å±•ï¼šè¯·æ±‚ä¹‹é—´åŠ ä¸€ç‚¹ delay
        delay_ms = TIMINGS_BASE_DELAY_MS + random.randint(0, TIMINGS_RANDOM_DELAY_MS)
        time.sleep(delay_ms / 1000.0)

        logger.info(f"timings: totals sent={self.timings_sent} ok={self.timings_ok} fail={self.timings_fail}")
        return ok

    # ----------------------------
    # Human-like reading (æŒ‰ä½ â€œå¯ç”¨è„šæœ¬â€çš„æ»šåŠ¨èŠ‚å¥)
    # ----------------------------
    def _read_like_human_with_scroll(self, page, seconds: float):
        """
        âœ… å®Œå…¨æŒ‰ä½ é‚£ä»½â€œèƒ½ç”¨è„šæœ¬â€çš„èŠ‚å¥ï¼š
        - æ¯æ¬¡ scrollBy 200~500
        - é—´éš” 1~3 ç§’
        - æœŸé—´è§¦å‘ scroll / focus / mousemoveï¼ˆåŠ ä¸€ç‚¹ï¼‰
        """
        end = time.time() + seconds
        while time.time() < end:
            dist = random.randint(READ_SCROLL_MIN, READ_SCROLL_MAX)
            try:
                page.run_js(
                    """
                    try { window.focus(); } catch(e) {}
                    try {
                      const ev = new MouseEvent('mousemove', {
                        clientX: 80 + Math.random()*600,
                        clientY: 80 + Math.random()*400
                      });
                      document.dispatchEvent(ev);
                    } catch(e) {}
                    try {
                      window.scrollBy(0, arguments[0]);
                      window.dispatchEvent(new Event('scroll'));
                    } catch(e) {}
                    """,
                    dist,
                )
            except Exception:
                pass
            time.sleep(random.uniform(READ_INTERVAL_MIN, READ_INTERVAL_MAX))

    def _read_post_like_human(self, page, post_id: int):
        """
        âœ… åªè¯»â€œä»æœ‰è“ç‚¹â€çš„æ¥¼å±‚ï¼š
        - æ»šåˆ°æ¥¼å±‚ä¸­é—´
        - æŒ‰çœŸäººèŠ‚å¥æ»šåŠ¨é˜…è¯» >= MIN_READ_STAY
        - é˜…è¯»åï¼šç”¨æ‰©å±•åŒæ¬¾ fetch æäº¤ timingsï¼ˆæ ¸å¿ƒï¼‰
        """
        try:
            page.run_js(
                """
                const pid = arguments[0];
                const el = document.querySelector(`#post_${pid}`);
                if (el) el.scrollIntoView({behavior:'instant', block:'center'});
                """,
                post_id,
            )
        except Exception:
            pass

        stay = max(MIN_READ_STAY, random.uniform(MIN_READ_STAY, MIN_READ_STAY + 6.0))
        logger.info(f"ğŸ‘€ é˜…è¯»æœªè¯»æ¥¼å±‚ post_{post_id}ï¼ˆé˜…è¯»æ»šåŠ¨â‰ˆ{stay:.1f}sï¼‰")

        # é˜…è¯»æ»šåŠ¨ï¼ˆæŒ‰ä½ é‚£ä»½â€œå¯ç”¨è„šæœ¬â€ï¼‰
        self._read_like_human_with_scroll(page, stay)

        # âœ… é˜…è¯»åï¼šä»è§†å£æ”¶é›†ä¸€æ‰¹æ¥¼å±‚å»æ‰“ timingsï¼ˆæ›´åƒä½ æŠ“åŒ…ï¼šä¸€æ¬¡ 3~6 ä¸ªæ¥¼å±‚ï¼‰
        vp = self._list_visible_posts_in_viewport(page)
        if ONLY_TIMINGS_UNREAD:
            vp = [pid for pid in vp if self._post_has_blue_dot(page, pid)]
        if not vp:
            # fallbackï¼šè‡³å°‘æŠŠå½“å‰ post_id æ‰“è¿›å»
            vp = [post_id]

        # æ§åˆ¶ batch sizeï¼Œæ¨¡ä»¿æ‰©å±•ï¼ˆminReq~maxReqï¼‰
        want = random.randint(TIMINGS_MIN_REQ, TIMINGS_MAX_REQ)
        batch = vp[:want]

        ok = self._post_timings_via_page_fetch(page, batch)
        if ok:
            return True

        # å¦‚æœå¤±è´¥ï¼Œä¸å†æ­»ç­‰ UIï¼›ä»…æç¤ºï¼ˆå› ä¸º UI ä¸ä¸€å®šåŒæ­¥ï¼‰
        logger.warning(
            f"âš ï¸ post_{post_id} æœ¬æ¬¡ timings æœªæˆåŠŸï¼ˆstatus!=200ï¼‰ï¼›UI æœªè§ read-state.read ä¸ä»£è¡¨ä¸€å®šæ²¡è®¡é˜…è¯»"
        )
        return False

    # ----------------------------
    # Near-bottom
    # ----------------------------
    def _near_bottom(self, page, gap=140) -> bool:
        try:
            return bool(
                page.run_js(
                    """
                    const d = document.documentElement;
                    const y = window.scrollY || d.scrollTop || 0;
                    const maxY = Math.max(0, d.scrollHeight - window.innerHeight);
                    return (maxY - y) <= arguments[0];
                    """,
                    gap,
                )
            )
        except Exception:
            return False

    # ----------------------------
    # Browse replies (5-10 pages) + åªè¯»è“ç‚¹æ¥¼å±‚ + timings
    # ----------------------------
    def browse_replies_pages(self, page, min_pages=5, max_pages=10):
        if max_pages < min_pages:
            max_pages = min_pages
        target_pages = random.randint(min_pages, max_pages)
        logger.info(f"ç›®æ ‡ï¼šæµè§ˆè¯„è®º {target_pages} é¡µï¼ˆæŒ‰æ¥¼å±‚å·å¢é•¿è®¡ï¼ŒPAGE_GROW={PAGE_GROW}ï¼‰")

        self.wait_topic_posts_ready(page, timeout=60)

        pages_done = 0
        last_max_no = self._max_post_number_in_dom(page)
        last_cnt = self._post_count_in_dom(page)
        logger.info(f"åˆå§‹ï¼šmax_post_no={last_max_no}, dom_posts={last_cnt}")

        max_loops = int(target_pages * 10 + 25)
        seen_read_attempts = set()

        for i in range(max_loops):
            # 1) å¤§æ­¥æ»šåŠ¨æ¨è¿›
            scroll_distance = random.randint(SCROLL_MIN, SCROLL_MAX)
            logger.info(f"[loop {i+1}] å‘ä¸‹æ»šåŠ¨ {scroll_distance}px æµè§ˆè¯„è®º...")
            try:
                page.run_js("window.scrollBy(0, arguments[0]);", scroll_distance)
            except Exception:
                pass

            # 2) ç­‰å¾…åŠ è½½
            time.sleep(random.uniform(1.0, 1.8))

            # 3) è§†å£å†…ï¼šåªè¯»â€œä»æœ‰è“ç‚¹â€çš„æ¥¼å±‚ï¼ˆæœ€å¤š 1~3 ä¸ªï¼‰
            vp = self._list_visible_posts_in_viewport(page)
            unread = [pid for pid in vp if self._post_has_blue_dot(page, pid)]
            unread = [pid for pid in unread if pid not in seen_read_attempts]

            if unread:
                k = min(len(unread), random.randint(1, 3))
                for pid in unread[:k]:
                    seen_read_attempts.add(pid)
                    self._read_post_like_human(page, pid)

            # 4) é¡µè®¡æ•°ï¼šmax_post_no å¢é•¿
            cur_max_no = self._max_post_number_in_dom(page)
            cur_cnt = self._post_count_in_dom(page)

            if cur_max_no - last_max_no >= PAGE_GROW:
                pages_done += 1
                logger.success(
                    f"âœ… ç¬¬ {pages_done}/{target_pages} é¡µï¼šmax_post_no {last_max_no} -> {cur_max_no}ï¼ˆdom_posts={cur_cnt}ï¼‰"
                )
                last_max_no = cur_max_no
                last_cnt = cur_cnt

            # 5) near-bottomï¼šé¢å¤–é˜…è¯»æ»šåŠ¨ï¼Œä¿ƒå‘åŠ è½½/è®¡æ—¶
            if self._near_bottom(page, gap=NEAR_BOTTOM_GAP):
                extra = random.uniform(BOTTOM_EXTRA_STAY_MIN, BOTTOM_EXTRA_STAY_MAX)
                logger.info(f"[loop {i+1}] æ¥è¿‘åº•éƒ¨ï¼ˆgap<={NEAR_BOTTOM_GAP}pxï¼‰ï¼Œé¢å¤–é˜…è¯»â‰ˆ{extra:.1f}s")
                self._read_like_human_with_scroll(page, extra)

            # 6) è¾¾æ ‡é€€å‡º
            if pages_done >= target_pages:
                logger.success("ğŸ‰ å·²è¾¾åˆ°ç›®æ ‡è¯„è®ºé¡µæ•°ï¼Œç»“æŸæµè§ˆ")
                return True

            # 7) åˆ°åº•åˆ¤æ–­
            try:
                at_bottom = page.run_js(
                    "return (window.scrollY + window.innerHeight) >= (document.body.scrollHeight - 5);"
                )
            except Exception:
                at_bottom = False

            if at_bottom:
                logger.success("å·²åˆ°è¾¾é¡µé¢åº•éƒ¨ï¼Œç»“æŸæµè§ˆ")
                if cur_max_no <= (min_pages * PAGE_GROW + 5):
                    logger.info(f"ä¸»é¢˜è¾ƒçŸ­ï¼ˆmax_post_noâ‰ˆ{cur_max_no}ï¼‰ï¼Œæ”¾å®½æœ€å°é¡µæ•°è¦æ±‚ï¼Œè§†ä¸ºå®Œæˆ")
                    return True
                return pages_done >= min_pages

        logger.warning("è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°ä»æœªå®Œæˆç›®æ ‡é¡µæ•°ï¼ˆå¯èƒ½åŠ è½½æ…¢/ä¸»é¢˜å¾ˆçŸ­/è¢«æ‹¦æˆªï¼‰")
        return pages_done >= min_pages

    # ----------------------------
    # Browse from latest list
    # ----------------------------
    def click_topic(self):
        if not self.page.url.startswith("https://linux.do/latest"):
            self.page.get(LIST_URL)

        if not self._wait_any_topic_link(timeout=35):
            logger.error("æœªæ‰¾åˆ° a.raw-topic-linkï¼ˆä¸»é¢˜æ ‡é¢˜é“¾æ¥ï¼‰")
            logger.error(f"å½“å‰URL: {self.page.url}")
            logger.error((self.page.html or "")[:500])
            return False

        topic_links = self.page.eles("css:a.raw-topic-link")
        if not topic_links:
            logger.error("ä¸»é¢˜é“¾æ¥åˆ—è¡¨ä¸ºç©º")
            logger.error(f"å½“å‰URL: {self.page.url}")
            logger.error((self.page.html or "")[:500])
            return False

        count = min(MAX_TOPICS, len(topic_links))
        logger.info(f"å‘ç° {len(topic_links)} ä¸ªä¸»é¢˜å¸–ï¼Œéšæœºé€‰æ‹© {count} ä¸ªè¿›è¡Œæµè§ˆ")

        for a in random.sample(topic_links, count):
            href = a.attr("href")
            if not href:
                continue
            if href.startswith("/"):
                href = "https://linux.do" + href
            self.click_one_topic(href)

        return True

    @retry_decorator()
    def click_one_topic(self, topic_url):
        new_page = self.browser.new_tab()
        try:
            new_page.get(topic_url)

            self.wait_topic_posts_ready(new_page, timeout=60)
            time.sleep(random.uniform(1.0, 2.0))

            if random.random() < LIKE_PROB:
                self.click_like(new_page)

            ok = self.browse_replies_pages(
                new_page,
                min_pages=MIN_COMMENT_PAGES,
                max_pages=MAX_COMMENT_PAGES,
            )
            if not ok:
                logger.warning("æœ¬ä¸»é¢˜æœªè¾¾åˆ°æœ€å°è¯„è®ºé¡µæ•°ç›®æ ‡ï¼ˆå¯èƒ½å¸–å­å¾ˆçŸ­/åˆ°åº•/åŠ è½½æ…¢ï¼‰")

        finally:
            try:
                new_page.close()
            except Exception:
                pass

    # ----------------------------
    # Like
    # ----------------------------
    def click_like(self, page):
        try:
            like_button = page.ele(".discourse-reactions-reaction-button")
            if like_button:
                logger.info("æ‰¾åˆ°æœªç‚¹èµçš„å¸–å­ï¼Œå‡†å¤‡ç‚¹èµ")
                like_button.click()
                logger.info("ç‚¹èµæˆåŠŸ")
                time.sleep(random.uniform(1, 2))
            else:
                logger.info("å¸–å­å¯èƒ½å·²ç»ç‚¹è¿‡èµäº†")
        except Exception as e:
            logger.error(f"ç‚¹èµå¤±è´¥: {str(e)}")

    # ----------------------------
    # Connect info
    # ----------------------------
    def print_connect_info(self):
        logger.info("è·å–è¿æ¥ä¿¡æ¯")
        headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        }
        resp = self.session.get(
            "https://connect.linux.do/",
            headers=headers,
            impersonate="chrome136",
            allow_redirects=True,
            timeout=30,
        )
        soup = BeautifulSoup(resp.text, "html.parser")
        rows = soup.select("table tr")
        info = []
        for row in rows:
            cells = row.select("td")
            if len(cells) >= 3:
                project = cells[0].text.strip()
                current = cells[1].text.strip() if cells[1].text.strip() else "0"
                requirement = cells[2].text.strip() if cells[2].text.strip() else "0"
                info.append([project, current, requirement])

        print("--------------Connect Info-----------------")
        print(tabulate(info, headers=["é¡¹ç›®", "å½“å‰", "è¦æ±‚"], tablefmt="pretty"))

    # ----------------------------
    # Notifications
    # ----------------------------
    def send_notifications(self, browse_enabled):
        status_msg = f"âœ…æ¯æ—¥ç™»å½•æˆåŠŸ: {USERNAME}"
        if browse_enabled:
            status_msg += (
                f" + æµè§ˆä»»åŠ¡å®Œæˆ(è¯é¢˜<= {MAX_TOPICS} ä¸ª, è¯„è®º{MIN_COMMENT_PAGES}-{MAX_COMMENT_PAGES}é¡µ, "
                f"PAGE_GROW={PAGE_GROW}, MIN_READ_STAY={MIN_READ_STAY}s, HEADLESS={HEADLESS}, DP_PORT={DP_PORT}, "
                f"timings_ok={self.timings_ok}/{self.timings_sent})"
            )

        if GOTIFY_URL and GOTIFY_TOKEN:
            try:
                response = requests.post(
                    f"{GOTIFY_URL}/message",
                    params={"token": GOTIFY_TOKEN},
                    json={"title": "LINUX DO", "message": status_msg, "priority": 1},
                    timeout=10,
                )
                response.raise_for_status()
                logger.success("æ¶ˆæ¯å·²æ¨é€è‡³Gotify")
            except Exception as e:
                logger.error(f"Gotifyæ¨é€å¤±è´¥: {str(e)}")
        else:
            logger.info("æœªé…ç½®Gotifyç¯å¢ƒå˜é‡ï¼Œè·³è¿‡é€šçŸ¥å‘é€")

        if SC3_PUSH_KEY:
            match = re.match(r"sct(\d+)t", SC3_PUSH_KEY, re.I)
            if not match:
                logger.error("âŒ SC3_PUSH_KEYæ ¼å¼é”™è¯¯ï¼Œæœªè·å–åˆ°UIDï¼Œæ— æ³•ä½¿ç”¨Serveré…±Â³æ¨é€")
                return
            uid = match.group(1)
            url = f"https://{uid}.push.ft07.com/send/{SC3_PUSH_KEY}"
            params = {"title": "LINUX DO", "desp": status_msg}

            attempts = 5
            for attempt in range(attempts):
                try:
                    response = requests.get(url, params=params, timeout=10)
                    response.raise_for_status()
                    logger.success(f"Serveré…±Â³æ¨é€æˆåŠŸ: {response.text}")
                    break
                except Exception as e:
                    logger.error(f"Serveré…±Â³æ¨é€å¤±è´¥: {str(e)}")
                    if attempt < attempts - 1:
                        sleep_time = random.randint(180, 360)
                        logger.info(f"å°†åœ¨ {sleep_time} ç§’åé‡è¯•...")
                        time.sleep(sleep_time)

        if WXPUSH_URL and WXPUSH_TOKEN:
            try:
                response = requests.post(
                    f"{WXPUSH_URL}/wxsend",
                    headers={
                        "Authorization": WXPUSH_TOKEN,
                        "Content-Type": "application/json",
                    },
                    json={"title": "LINUX DO", "content": status_msg},
                    timeout=10,
                )
                response.raise_for_status()
                logger.success(f"wxpush æ¨é€æˆåŠŸ: {response.text}")
            except Exception as e:
                logger.error(f"wxpush æ¨é€å¤±è´¥: {str(e)}")
        else:
            logger.info("æœªé…ç½® WXPUSH_URL æˆ– WXPUSH_TOKENï¼Œè·³è¿‡é€šçŸ¥å‘é€")

    # ----------------------------
    # Run
    # ----------------------------
    def run(self):
        try:
            login_res = self.login()
            if not login_res:
                logger.warning("ç™»å½•å¤±è´¥ï¼Œåç»­ä»»åŠ¡å¯èƒ½æ— æ³•è¿›è¡Œ")

            if BROWSE_ENABLED:
                click_topic_res = self.click_topic()
                if not click_topic_res:
                    logger.error("ç‚¹å‡»ä¸»é¢˜å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
                    return
                logger.info("å®Œæˆæµè§ˆä»»åŠ¡ï¼ˆå«è¯„è®ºæµè§ˆï¼‰")

            self.send_notifications(BROWSE_ENABLED)

        finally:
            try:
                self.page.close()
            except Exception:
                pass
            try:
                self.browser.quit()
            except Exception:
                pass


if __name__ == "__main__":
    if not USERNAME or not PASSWORD:
        print("Please set LINUXDO_USERNAME/LINUXDO_PASSWORD (or USERNAME/PASSWORD)")
        raise SystemExit(1)

    l = LinuxDoBrowser()
    l.run()
